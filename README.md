# sdse_rgofv
Welcome to the RGoOFV MLOps project repo! 
Please note that this repo contains two workflow solutions, one using the Cookiecutter Data Science template with a CLI-implementation using Typer commands, and one using decomposed python scripts of the original Notebook, run through a Go pipeline.
Both approaches create the necessary files and pass the validation check on git Workflow using an automated Dagger pipeline.
## Project Organization
├──.github
    ├──workflows   
        ├── dagger.yml
├── artifacts <- all artifacts generated by pipeline.go
    ├── ...
    ├── ...
├──cookiecutter_pipeline/itu-sdse-project-rgofv <- 
    ├── Makefile           <- Makefile with convenience commands like `make data` or `make train`
    ├── README.md   
    ├── base_data <- indsæt kommentar
        ├── .dvc
        ├── .gitignore      
        ├── raw_data.csv.dvc   
    ├── mlops_pipeline            <- Source folder
        ├── cli  
            ├── app.py  
        ├── data
            ├── __init__.py
            ├── io.py
            ├── make_dataset.py
            ├── prepare.py  
        ├── features
            ├── __init__.py
            ├── build_features.py
        ├── modeling
            ├── __init__.py
            ├── logreg.py
            ├── predict.py
            ├── save_best_model.py
            ├── train_py
            ├── xgboost_rf.py
        ├── utils
            ├── __init__.py
            ├── mlflow_utils.py
            ├── save_artifacts.py
        ├── __init__.py
        ├── config.py
        ├── dataset.py 
        ├── features_cli.py
        ├── full_pipeline.py
        ├── plots.py  
    │
    ├── models          <- Trained and serialized models, model predictions, or model summaries  
        ├── artifacts
            ├── column_list.json
            ├── model_results.json
        ├── .gitkeep
        ├── classification_reports...
        ├── lead_model...
        ├── scaler.pkl
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-jqp-initial-data-exploration`.
    │
    ├── pyproject.toml     <- Project configuration file with package metadata for 
    │                         mlops_pipeline
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.

    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
├── data <- dvc pointer
    - .dvc
├── mlflow 
├── mlruns
├── .daggerignore <- 
├── go.mod
├── go.sum
├── pipeline.go
├── README.md
├── requirements.txt

Running pipeline:

To run the project a virtual envoirement with the following dependencies are needed:
    - dagger >= v0.19.8
    - go >= go1.25.0
    - docker >= 28.3.2
    - python >= 3.10
The dagger pipeline will install all local dependencies needed when running, 

When dependencies are setup, navigate to the root of the repo and run the following command:
    dagger run go pipeline.go

To trigger the Github Automation flow:
    - Must push to the following branch:
        - "main"


To run the CLI, go to cookiecutter_pipeline/itu-sdse-project-rgofv and run “pip install -e .” in your virtual environment. 
Then you can run the command “mlops –help” to see possible commands.

